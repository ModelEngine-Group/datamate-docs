---
title: 数据归集
description: 使用 DataMate 从多种数据源归集数据
weight: 1
---

{{% pageinfo %}}
数据归集模块帮助您从多种数据源（数据库、文件系统、API 等）归集数据到 DataMate 平台。
{{% /pageinfo %}}

## 功能概述

数据归集模块基于 [DataX](https://github.com/alibaba/DataX) 实现，支持：

- **多种数据源**：MySQL、PostgreSQL、Oracle、SQL Server 等关系型数据库
- **异构同步**：不同数据源之间的数据同步
- **批量归集**：大规模数据的批量归集和同步
- **定时任务**：支持定时执行归集任务
- **任务监控**：实时监控归集任务执行状态

## 支持的数据源

| 数据源类型 | Reader | Writer | 说明 |
|-----------|--------|--------|------|
| MySQL | ✅ | ✅ | 关系型数据库 |
| PostgreSQL | ✅ | ✅ | 关系型数据库 |
| Oracle | ✅ | ✅ | 企业级数据库 |
| SQL Server | ✅ | ✅ | 微软数据库 |
| JSON 文件 | ✅ | ✅ | JSON 格式文件 |
| CSV 文件 | ✅ | ✅ | CSV 格式文件 |
| TXT 文件 | ✅ | ✅ | 文本文件 |
| FTP | ✅ | ✅ | FTP 服务器 |
| HDFS | ✅ | ✅ | Hadoop 分布式文件系统 |

## 快速开始

### 1. 创建采集任务

#### 步骤 1：进入数据采集页面

在左侧导航栏选择 **数据** → **数据采集**。

#### 步骤 2：点击创建任务

点击右上角 **创建任务** 按钮。

#### 步骤 3：配置基本信息

填写以下基本信息：

- **名称**：为任务取一个有意义的名称
- **超时时间**：任务执行超时时间（秒）
- **描述**：描述任务用途（可选）

#### 步骤 4：选择同步方式

选择任务的同步方式：

- **立即同步**：创建任务后立即执行一次
- **定时同步**：按照定时规则周期性执行

#### 步骤 5：配置数据源

**选择数据源类型**：从下拉列表中选择数据源类型（如 MySQL、CSV 等）

**配置数据源参数**：根据选择的数据源模板，填写相应的连接参数（表单形式）

**MySQL 示例**：
- JDBC URL：`jdbc:mysql://localhost:3306/mydb`
- 用户名：`root`
- 密码：`password`
- 表名：`users`

**CSV 文件示例**：
- 文件路径：`/data/input.csv`
- 编码：`UTF-8`
- 分隔符：`,`
- 包含表头：是/否

#### 步骤 6：配置字段映射

配置源字段到目标字段的映射关系：

| 源字段 | 目标字段 | 类型转换 |
|--------|---------|----------|
| id | user_id | Long → Long |
| name | username | String → String |
| email | email | String → String |

#### 步骤 7：配置执行策略

- **并发数**：任务并发通道数（默认 1）

选择 **定时同步** 时，需要配置定时规则：

- **Cron 表达式**：如 `0 0 2 * * ?` 表示每天凌晨 2 点执行
- 或使用表单快速设置：
  - 执行频率：每小时 / 每天 / 每周 / 每月
  - 具体时间：选择执行时间点

#### 步骤 8：创建并执行

点击 **创建** 按钮创建任务。
- 如果选择 **立即同步**，任务将自动开始运行
- 如果选择 **定时同步**，任务将按照定时规则周期性执行

### 2. 监控任务执行

#### 查看任务列表

在数据采集页面，可以看到所有采集任务及其状态：

| 任务名称 | 数据源 | 状态 | 最后执行时间 | 操作 |
|---------|--------|------|-------------|------|
| 用户数据归集 | MySQL | 运行中 | 2024-01-15 10:30 | 查看执行记录 |
| 日志文件归集 | CSV | 已完成 | 2024-01-14 18:00 | 查看执行记录 |

#### 查看任务详情

点击任务名称或 **查看详情** 按钮，可以看到：

- **基本信息**：任务配置信息
- **执行记录**：历史执行记录
- **执行日志**：详细的执行日志
- **数据统计**：采集的数据量、成功/失败记录数

#### 查看执行日志

在任务详情页面，点击 **执行记录** 标签，选择某次执行记录，查看详细日志：

```
2024-01-15 10:30:00 INFO  任务开始执行
2024-01-15 10:30:01 INFO  连接数据源成功
2024-01-15 10:30:02 INFO  开始读取数据
2024-01-15 10:30:10 INFO  读取数据完成，共 10000 条
2024-01-15 10:30:11 INFO  开始写入目标数据集
2024-01-15 10:30:15 INFO  写入数据完成
2024-01-15 10:30:15 INFO  任务执行成功
```

### 3. 任务管理

任务列表中每个任务的操作包括：

- **查看执行记录**：查看任务的历次执行情况
- **删除**：删除任务（注意：删除任务不会删除已归集的数据）

点击任务名称可以查看任务详情，包括：
- 基本信息配置
- 执行记录列表
- 数据统计信息

## 高级功能

### DataX 配置模板

DataMate 支持使用 DataX JSON 配置模板创建任务。

#### 基本结构

```json
{
  "job": {
    "content": [
      {
        "reader": {
          "name": "mysqlreader",
          "parameter": {
            "username": "root",
            "password": "password",
            "column": ["id", "name", "email"],
            "connection": [
              {
                "jdbcUrl": ["jdbc:mysql://localhost:3306/mydb"],
                "table": ["users"]
              }
            ]
          }
        },
        "writer": {
          "name": "txtfilewriter",
          "parameter": {
            "path": "/data/output",
            "fileName": "users",
            "writeMode": "truncate"
          }
        }
      }
    ],
    "setting": {
      "speed": {
        "channel": 1,
        "byte": 1048576
      }
    }
  }
}
```

#### 使用模板

1. 在创建任务时，选择 **使用模板**
2. 粘贴 DataX JSON 配置
3. 系统会自动解析配置并填充表单

### 定时任务

使用 Cron 表达式配置定时执行规则：

| 表达式 | 说明 |
|--------|------|
| `0 0 2 * * ?` | 每天凌晨 2 点执行 |
| `0 0 */2 * * ?` | 每 2 小时执行一次 |
| `0 0 12 * * ?` | 每天中午 12 点执行 |
| `0 0 0 ? * MON` | 每周一凌晨执行 |
| `0 0 0 1 * ?` | 每月 1 号凌晨执行 |

### 并发控制

对于大规模数据采集，可以配置并发参数：

| 参数 | 说明 | 推荐值 |
|------|------|--------|
| channel | 并发通道数 | 1-5 |
| byte | 字节流限制 | 1048576 (1MB) |
| record | 记录流限制 | 100000 |

### 数据转换

支持在采集过程中进行简单的数据转换：

- **类型转换**：String → Long、Double → String 等
- **日期格式化**：将日期字符串转换为标准格式
- **字符串处理**：trim、substring 等
- **空值处理**：设置默认值

## 常见问题

### Q: 任务执行失败怎么办？

A: 按以下步骤排查：

1. **检查数据源连接**：确保数据源地址、端口、用户名、密码正确
2. **检查网络连接**：确保 DataMate 能访问数据源
3. **查看执行日志**：获取详细错误信息
4. **检查数据格式**：确保数据格式与配置一致
5. **检查目标数据集**：确保目标数据集存在且有写入权限

### Q: 如何采集大表数据？

A: 对于大表数据采集：

1. **使用增量采集**：配置时间字段进行增量同步
2. **分批采集**：将大表拆分为多个小任务
3. **调整并发参数**：适当增加 channel 数量
4. **使用过滤条件**：只采集需要的数据

### Q: 如何实现增量采集？

A: 配置增量采集条件：

在数据源配置中，可以设置查询条件（如 SQL WHERE 子句），例如：

```sql
WHERE update_time > '${lastTime}'
```

系统会在每次执行时记录最后一次执行时间，下次执行时自动使用该时间进行增量采集。

### Q: 采集速度慢怎么办？

A: 优化采集速度：

1. **增加并发通道**：将 channel 设置为 3-5
2. **调整流控参数**：增加 byte 和 record 限制
3. **优化 SQL 查询**：使用索引、减少查询字段
4. **使用批量写入**：启用批量写入模式

### Q: 如何处理数据类型不兼容？

A: 使用类型转换：

```json
{
  "column": [
    {
      "name": "id",
      "type": "long"
    },
    {
      "name": "price",
      "type": "decimal",
      "format": "#.##"
    }
  ]
}
```

## API 参考

详细的 API 文档请参考：
- [数据采集 API](/docs/api-reference/data-collection/)

## 相关文档

- [数据管理](/docs/user-guide/data-management/) - 采集后的数据管理
- [数据清洗](/docs/user-guide/data-cleansing/) - 采集后的数据清洗
- [流水线编排](/docs/user-guide/orchestration/) - 将采集任务集成到流水线
