---
title: 数据归集
description: 使用 DataMate 从多种数据源归集数据
weight: 1
---

{{% pageinfo %}}
数据归集模块帮助您从多种数据源（数据库、文件系统、API 等）归集数据到 DataMate 平台。
{{% /pageinfo %}}

## 功能概述

数据归集模块基于 [DataX](https://github.com/alibaba/DataX) 实现，支持：

- **多种数据源**：MySQL、PostgreSQL、Oracle、SQL Server 等关系型数据库
- **异构同步**：不同数据源之间的数据同步
- **批量归集**：大规模数据的批量归集和同步
- **定时任务**：支持定时执行归集任务
- **任务监控**：实时监控归集任务执行状态

## 支持的数据源

| 数据源类型 | Reader | Writer | 说明 |
|-----------|--------|--------|------|
| 通用关系型数据库 | ✅ | ✅ | 支持 MySQL、PostgreSQL、OpenGauss、SQL Server、达梦、DB2 |
| MySQL | ✅ | ✅ | 关系型数据库 |
| PostgreSQL | ✅ | ✅ | 关系型数据库 |
| OpenGauss | ✅ | ✅ | 关系型数据库 |
| SQL Server | ✅ | ✅ | 微软数据库 |
| 达梦 | ✅ | ✅ | 国产数据库 |
| DB2 | ✅ | ✅ | IBM 数据库 |
| StarRocks | ✅ | ✅ | 分析型数据库 |
| NAS | ✅ | ✅ | 网络存储 |
| S3 | ✅ | ✅ | 对象存储 |
| GlusterFS | ✅ | ✅ | 分布式文件系统 |
| API 归集 | ✅ | ✅ | API 接口数据 |
| JSON 文件 | ✅ | ✅ | JSON 格式文件 |
| CSV 文件 | ✅ | ✅ | CSV 格式文件 |
| TXT 文件 | ✅ | ✅ | 文本文件 |
| FTP | ✅ | ✅ | FTP 服务器 |
| HDFS | ✅ | ✅ | Hadoop 分布式文件系统 |

## 快速开始

### 1. 创建采集任务

#### 步骤 1：进入数据归集页面

在左侧导航栏选择 **数据归集**。

#### 步骤 2：点击创建任务

点击右上角 **创建任务** 按钮。

#### 步骤 3：配置基本信息

填写以下基本信息：

- **名称**：为任务取一个有意义的名称
- **超时时间**：任务执行超时时间（秒）
- **描述**：描述任务用途（可选）

#### 步骤 4：选择同步方式

选择任务的同步方式：

- **立即同步**：创建任务后立即执行一次
- **定时同步**：按照定时规则周期性执行

选择 **定时同步** 时，需要配置执行策略：

- **执行周期**：每小时 / 每天 / 每周 / 每月
- **执行时间**：选择执行时间点

#### 步骤 5：配置数据源

**选择数据源类型**：从下拉列表中选择数据源类型（如 MySQL、CSV 等）

**配置数据源参数**：根据选择的数据源模板，填写相应的连接参数（表单形式）

**MySQL 示例**：
- JDBC URL：`jdbc:mysql://localhost:3306/mydb`
- 用户名：`root`
- 密码：`password`
- 表名：`users`

#### 步骤 6：配置字段提取

系统不支持配置字段映射，只能从配置的 SQL 中提取部分字段。

- **提取指定字段**：在字段列表中填写需要提取的字段名称
- **提取所有字段**：不填写字段名称，则提取 SQL 查询结果中的所有字段

#### 步骤 7：创建并执行

点击 **创建** 按钮创建任务。
- 如果选择 **立即同步**，任务将自动开始运行
- 如果选择 **定时同步**，任务将按照定时规则周期性执行

### 2. 监控任务执行

#### 查看任务列表

在数据采集页面，可以看到所有采集任务及其状态：

| 任务名称 | 数据源 | 状态 | 最后执行时间 | 操作 |
|---------|--------|------|-------------|------|
| 用户数据归集 | MySQL | 运行中 | 2024-01-15 10:30 | 查看执行记录 |
| 日志文件归集 | CSV | 已完成 | 2024-01-14 18:00 | 查看执行记录 |

#### 查看任务详情

点击任务名称或 **查看详情** 按钮，可以看到：

- **基本信息**：任务配置信息
- **执行记录**：历史执行记录
- **执行日志**：详细的执行日志
- **数据统计**：采集的数据量、成功/失败记录数

#### 查看执行日志

在任务详情页面，点击 **执行记录** 标签，选择某次执行记录，查看详细日志：

```
2024-01-15 10:30:00 INFO  任务开始执行
2024-01-15 10:30:01 INFO  连接数据源成功
2024-01-15 10:30:02 INFO  开始读取数据
2024-01-15 10:30:10 INFO  读取数据完成，共 10000 条
2024-01-15 10:30:11 INFO  开始写入目标数据集
2024-01-15 10:30:15 INFO  写入数据完成
2024-01-15 10:30:15 INFO  任务执行成功
```

### 3. 任务管理

任务列表中每个任务的操作包括：

- **查看执行记录**：查看任务的历次执行情况
- **删除**：删除任务（注意：删除任务不会删除已归集的数据）

点击任务名称可以查看任务详情，包括：
- 基本信息配置
- 执行记录列表
- 数据统计信息

## 常见问题

### Q: 任务执行失败怎么办？

A: 按以下步骤排查：

1. **检查数据源连接**：确保数据源地址、端口、用户名、密码正确
2. **检查网络连接**：确保 DataMate 能访问数据源
3. **查看执行日志**：获取详细错误信息
4. **检查数据格式**：确保数据格式与配置一致
5. **检查目标数据集**：确保目标数据集存在且有写入权限

### Q: 如何采集大表数据？

A: 对于大表数据采集：

1. **使用增量采集**：配置时间字段进行增量同步
2. **分批采集**：将大表拆分为多个小任务
3. **调整并发参数**：适当增加 channel 数量
4. **使用过滤条件**：只采集需要的数据

### Q: 如何实现增量采集？

A: 配置增量采集条件：

在数据源配置中，可以设置查询条件（如 SQL WHERE 子句），例如：

```sql
WHERE update_time > '${lastTime}'
```

系统会在每次执行时记录最后一次执行时间，下次执行时自动使用该时间进行增量采集。

### Q: 采集速度慢怎么办？

A: 优化采集速度：

1. **增加并发通道**：将 channel 设置为 3-5
2. **调整流控参数**：增加 byte 和 record 限制
3. **优化 SQL 查询**：使用索引、减少查询字段
4. **使用批量写入**：启用批量写入模式

### Q: 如何处理数据类型不兼容？

A: 使用类型转换：

```json
{
  "column": [
    {
      "name": "id",
      "type": "long"
    },
    {
      "name": "price",
      "type": "decimal",
      "format": "#.##"
    }
  ]
}
```

## API 参考

详细的 API 文档请参考：
- [数据采集 API](/docs/api-reference/data-collection/)

## 相关文档

- [数据管理](/docs/user-guide/data-management/) - 采集后的数据管理
- [数据清洗](/docs/user-guide/data-cleansing/) - 采集后的数据清洗
- [流水线编排](/docs/user-guide/orchestration/) - 将采集任务集成到流水线
