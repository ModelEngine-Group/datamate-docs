---
title: 数据采集
description: 使用 DataMate 从多种数据源采集数据
weight: 1
---

{{% pageinfo %}}
数据采集模块帮助您从多种数据源（数据库、文件系统、API 等）采集数据到 DataMate 平台。
{{% /pageinfo %}}

## 功能概述

数据采集模块基于 [DataX](https://github.com/alibaba/DataX) 实现，支持：

- **多种数据源**：MySQL、PostgreSQL、Oracle、SQL Server 等关系型数据库
- **异构同步**：不同数据源之间的数据同步
- **批量采集**：大规模数据的批量采集和同步
- **定时任务**：支持定时执行采集任务
- **任务监控**：实时监控采集任务执行状态

## 支持的数据源

| 数据源类型 | Reader | Writer | 说明 |
|-----------|--------|--------|------|
| MySQL | ✅ | ✅ | 关系型数据库 |
| PostgreSQL | ✅ | ✅ | 关系型数据库 |
| Oracle | ✅ | ✅ | 企业级数据库 |
| SQL Server | ✅ | ✅ | 微软数据库 |
| JSON 文件 | ✅ | ✅ | JSON 格式文件 |
| CSV 文件 | ✅ | ✅ | CSV 格式文件 |
| TXT 文件 | ✅ | ✅ | 文本文件 |
| FTP | ✅ | ✅ | FTP 服务器 |
| HDFS | ✅ | ✅ | Hadoop 分布式文件系统 |

## 快速开始

### 1. 创建采集任务

#### 步骤 1：进入数据采集页面

在左侧导航栏选择 **数据** → **数据采集**。

#### 步骤 2：点击创建任务

点击右上角 **创建任务** 按钮。

#### 步骤 3：配置基本信息

- **任务名称**：为任务取一个有意义的名称
- **任务描述**：描述任务用途（可选）
- **数据源类型**：选择数据源类型（如 MySQL、CSV 等）
- **目标数据集**：选择或创建目标数据集

#### 步骤 4：配置数据源连接

根据选择的数据源类型，填写相应的连接信息：

**MySQL 示例**：
```json
{
  "host": "localhost",
  "port": 3306,
  "username": "root",
  "password": "password",
  "database": "mydb",
  "table": "users"
}
```

**CSV 文件示例**：
```json
{
  "path": "/data/input.csv",
  "encoding": "UTF-8",
  "delimiter": ",",
  "hasHeader": true
}
```

#### 步骤 5：配置字段映射

配置源字段到目标字段的映射关系：

| 源字段 | 目标字段 | 类型转换 |
|--------|---------|----------|
| id | user_id | Long → Long |
| name | username | String → String |
| email | email | String → String |

#### 步骤 6：配置执行策略

- **执行模式**：立即执行 / 定时执行
- **定时规则**：Cron 表达式（如 `0 0 2 * * ?` 每天凌晨 2 点执行）
- **并发数**：任务并发数（默认 1）

#### 步骤 7：创建并执行

点击 **创建** 按钮创建任务。如果选择立即执行，任务将自动开始运行。

### 2. 监控任务执行

#### 查看任务列表

在数据采集页面，可以看到所有采集任务及其状态：

| 任务名称 | 数据源 | 状态 | 最后执行时间 | 操作 |
|---------|--------|------|-------------|------|
| 用户数据采集 | MySQL | 运行中 | 2024-01-15 10:30 | 查看详情 |
| 日志文件采集 | CSV | 已完成 | 2024-01-14 18:00 | 查看详情 |

#### 查看任务详情

点击任务名称或 **查看详情** 按钮，可以看到：

- **基本信息**：任务配置信息
- **执行记录**：历史执行记录
- **执行日志**：详细的执行日志
- **数据统计**：采集的数据量、成功/失败记录数

#### 查看执行日志

在任务详情页面，点击 **执行记录** 标签，选择某次执行记录，查看详细日志：

```
2024-01-15 10:30:00 INFO  任务开始执行
2024-01-15 10:30:01 INFO  连接数据源成功
2024-01-15 10:30:02 INFO  开始读取数据
2024-01-15 10:30:10 INFO  读取数据完成，共 10000 条
2024-01-15 10:30:11 INFO  开始写入目标数据集
2024-01-15 10:30:15 INFO  写入数据完成
2024-01-15 10:30:15 INFO  任务执行成功
```

### 3. 任务管理

#### 编辑任务

点击任务列表右侧的 **编辑** 按钮，可以修改任务配置。

注意：只有 **未执行** 或 **已完成** 状态的任务可以编辑。

#### 删除任务

点击任务列表右侧的 **删除** 按钮，可以删除任务。

注意：删除任务不会删除已采集的数据。

#### 暂停/恢复任务

对于 **运行中** 的任务，可以点击 **暂停** 按钮暂停任务执行。

点击 **恢复** 按钮可以恢复暂停的任务。

## 高级功能

### DataX 配置模板

DataMate 支持使用 DataX JSON 配置模板创建任务。

#### 基本结构

```json
{
  "job": {
    "content": [
      {
        "reader": {
          "name": "mysqlreader",
          "parameter": {
            "username": "root",
            "password": "password",
            "column": ["id", "name", "email"],
            "connection": [
              {
                "jdbcUrl": ["jdbc:mysql://localhost:3306/mydb"],
                "table": ["users"]
              }
            ]
          }
        },
        "writer": {
          "name": "txtfilewriter",
          "parameter": {
            "path": "/data/output",
            "fileName": "users",
            "writeMode": "truncate"
          }
        }
      }
    ],
    "setting": {
      "speed": {
        "channel": 1,
        "byte": 1048576
      }
    }
  }
}
```

#### 使用模板

1. 在创建任务时，选择 **使用模板**
2. 粘贴 DataX JSON 配置
3. 系统会自动解析配置并填充表单

### 定时任务

使用 Cron 表达式配置定时执行规则：

| 表达式 | 说明 |
|--------|------|
| `0 0 2 * * ?` | 每天凌晨 2 点执行 |
| `0 0 */2 * * ?` | 每 2 小时执行一次 |
| `0 0 12 * * ?` | 每天中午 12 点执行 |
| `0 0 0 ? * MON` | 每周一凌晨执行 |
| `0 0 0 1 * ?` | 每月 1 号凌晨执行 |

### 并发控制

对于大规模数据采集，可以配置并发参数：

| 参数 | 说明 | 推荐值 |
|------|------|--------|
| channel | 并发通道数 | 1-5 |
| byte | 字节流限制 | 1048576 (1MB) |
| record | 记录流限制 | 100000 |

### 数据转换

支持在采集过程中进行简单的数据转换：

- **类型转换**：String → Long、Double → String 等
- **日期格式化**：将日期字符串转换为标准格式
- **字符串处理**：trim、substring 等
- **空值处理**：设置默认值

## 常见问题

### Q: 任务执行失败怎么办？

A: 按以下步骤排查：

1. **检查数据源连接**：确保数据源地址、端口、用户名、密码正确
2. **检查网络连接**：确保 DataMate 能访问数据源
3. **查看执行日志**：获取详细错误信息
4. **检查数据格式**：确保数据格式与配置一致
5. **检查目标数据集**：确保目标数据集存在且有写入权限

### Q: 如何采集大表数据？

A: 对于大表数据采集：

1. **使用增量采集**：配置时间字段进行增量同步
2. **分批采集**：将大表拆分为多个小任务
3. **调整并发参数**：适当增加 channel 数量
4. **使用过滤条件**：只采集需要的数据

### Q: 如何实现增量采集？

A: 配置增量采集条件：

```json
{
  "querySql": "SELECT * FROM users WHERE update_time > '${lastTime}'"
}
```

系统会在每次执行时记录最后一次执行时间，下次执行时自动使用该时间进行增量采集。

### Q: 采集速度慢怎么办？

A: 优化采集速度：

1. **增加并发通道**：将 channel 设置为 3-5
2. **调整流控参数**：增加 byte 和 record 限制
3. **优化 SQL 查询**：使用索引、减少查询字段
4. **使用批量写入**：启用批量写入模式

### Q: 如何处理数据类型不兼容？

A: 使用类型转换：

```json
{
  "column": [
    {
      "name": "id",
      "type": "long"
    },
    {
      "name": "price",
      "type": "decimal",
      "format": "#.##"
    }
  ]
}
```

## API 参考

详细的 API 文档请参考：
- [数据采集 API](/docs/api-reference/data-collection/)

## 相关文档

- [数据管理](/docs/user-guide/data-management/) - 采集后的数据管理
- [数据清洗](/docs/user-guide/data-cleansing/) - 采集后的数据清洗
- [流水线编排](/docs/user-guide/orchestration/) - 将采集任务集成到流水线
